# TurtleAutonomy: 차선 기반 자율주행 + ArUco 작업 수행 서비스 로봇

**ROS 2 + OpenCV 기반으로, 차선 인식 → 자율주행 → ArUco 마커 감지 후 작업 수행까지 연결되는 모바일 매니퓰레이터 시스템을 구현했습니다.**
**차선 기반 AGV 주행 + 로봇팔 조작을 연계하여 `이동 + 조작`이 가능한 복합 서비스 로봇 프로토타입을 목표로 전체 파이프라인을 설계하고 구현했습니다.**

* 📅 **개발 기간**: 2025.06.09 \~ 06.19 (11일)
* 🧑‍🤝‍🧑 **인원 구성**: 3명
  **(본인 역할: 시뮬레이션 환경 제작, GUI 설계, 차선 인식 알고리즘 및 BEV 변환, 이미지 전처리 성능 개선)**
* 🛠 **사용 기술**: ROS 2, OpenCV, ArUco, MoveIt, RViz, Python, Ubuntu 22.04
* ✅ **주요 기능**: 차선 인식 기반 자율주행, ArUco 마커 인식, 매니퓰레이터 제어, GUI 인터페이스

### 🎥 시연 영상

[![시연 영상 보기](https://img.youtube.com/vi/BHgXm7nOlXM/0.jpg)](https://youtu.be/BHgXm7nOlXM)

📄 [PDF 소개 자료](Turtobot.pdf)  

---

## 📑 프로젝트 개요

* **주제**: TurtleAutonomy (모바일 매니퓰레이터 서비스 로봇)
* **목표**: 차선 기반 자율주행 + ArUco 기반 물체 인식 및 작업 수행 자동화
* **형태**: TurtleBot3 Waffle + 매니퓰레이터 + 카메라 결합 모바일 로봇

**🧑‍🤝‍🧑 팀원 역할 분담**

> | 이름           | 역할                                        |
> | ------------ | ----------------------------------------- |
> | **이세현(작성자)** | **차선 인식 및 전처리 알고리즘 구현, 시뮬 환경 구성, GUI 제작** |
> | 강인우          | 시뮬레이션 제어 태스크, 이미지 전처리, ROS2 기반 제어 시스템 구현  |
> | 이형연          | Aruco 마커 인식, 매니퓰레이터 제어 로직 및 표지판 인식 학습     |

<br>

**📅 작업 일정**

> | 기간           | 작업 내용                        |
> | ------------ | ---------------------------- |
> | 6/09         | 기획 및 기능 설계                   |
> | 6/10 \~ 6/13 | 시뮬레이션 테스트 및 차선 인식/주행 구현      |
> | 6/14 \~ 6/18 | ArUco 인식 및 로봇팔 제어 연동, 통합 테스트 |
> | 6/19         | GUI 구성, 영상 시연 촬영 및 보고서 작성    |

---

## 1. 개발 배경 및 기획

차선 인식 기반 자율주행은 단순한 라인트레이서 이상의 응용 가능성을 가지며, 이를 모바일 매니퓰레이터와 결합할 경우 단일 로봇으로 `이동 + 작업`이 가능한 복합 서비스가 가능해집니다.

**TurtleAutonomy 프로젝트**는 실내 환경에서 AGV 역할을 하는 TurtleBot3에 로봇팔을 탑재하고, ArUco 마커 기반 작업 수행 기능을 더해 다양한 현장 시나리오에서 대응 가능한 서비스 로봇 시스템을 구현하는 것이 목표였습니다.

### 🔧 핵심 기능

1. 차선(가이드라인) 인식 기반 자율주행 (라인 트래킹)
2. ArUco 마커 인식 기반 위치 추정 및 작업 지시
3. 로봇팔 + 그리퍼 제어를 통한 Pick & Place 수행

### 🧭 프로세스

![모바일 매니퓰레이터 동작 구조](https://github.com/user-attachments/assets/example_turtlebot_structure.jpg)

---

## 2. 사용 장비 & 기술 스택

| 분류     | 기술                                     |
| ------ | -------------------------------------- |
| 언어     | Python                                 |
| 로봇 플랫폼 | TurtleBot3 Waffle + 매니퓰레이터             |
| OS     | Ubuntu 22.04                           |
| 주행 제어  | ROS 2, RViz, Gazebo, /cmd\_vel, MoveIt |
| 비전 인식  | OpenCV + ArUco + 카메라 (C270)            |
| 차선 인식  | CLAHE + Histogram Stretch + BEV 변환     |
| GUI    | PyQt 기반 이미지 디스플레이                      |

---

## 3. 주요 기능 구현 과정

### 3-1. 🛣️ 차선 인식 및 자율주행

#### 📌 기술 스택 및 구조

* CLAHE + 히스토그램 스트레칭 조합으로 밝기 변화 대응
* HSV 변환 및 마스킹 후 차선 중심 추출
* BEV(Bird Eye View) 변환 → 선형 라인 피팅 → 중심 오차 계산

#### ⚙️ 구현 내용

* CLAHE로 지역 대비 향상 → 전체 대비 보정 (히스토그램 스트레칭)
* BEV 시점에서 차선 곡선 np.polyfit() → 중심선 x좌표 계산
* 중심선과 화면 중심 오차로 PD 제어값 계산 → `/cmd_vel`로 속도 명령

---

### 3-2. ⚙️ PD 제어 기반 자율주행

* 중심 오차가 작을수록 직진, 클수록 회전값 증가
* 횡단보도 인식(픽셀 수 + 띠 개수 기준) 시 주행 정지
* 이미지 동기화(`message_filters`)로 입력 프레임 정합

---

### 3-3. 🧿 ArUco 마커 인식 및 자세 추정

#### 📌 기술 스택 및 구조

* OpenCV ArUco 모듈 + 카메라 파라미터 보정
* rvec, tvec 기반으로 마커의 거리 및 방향 추정
* Rodrigues 변환으로 오일러 각 추출 (roll, pitch, yaw)

#### ⚙️ 구현 내용

* ArUco 마커가 감지되면 가장 가까운 마커를 추적
* 위치 정보(tvec)와 방향(rvec)을 기반으로 로봇팔 작업 위치로 이동

---

### 3-4. 🤖 매니퓰레이터 제어 및 Pick & Place

#### 📌 기술 스택 및 구조

* MoveIt + ROS 2 비동기 서비스 요청 구조
* cmd 값을 통해 로봇팔 동작/그리퍼 제어 분기

#### ⚙️ 구현 내용

1. 그리퍼 열기
2. 마커 위로 이동
3. 집을 수 있는 위치로 이동
4. 그리퍼 닫기
5. 위로 올리기
6. 컨베이어 상단 위치 이동
7. 적재 위치 이동
8. 그리퍼 열기
9. 복귀

---

## 4. 핵심 코드 구현

### `lane_detector.py`

**역할**: 차선 중심 계산 및 BEV 이미지 생성
**기능**: CLAHE → Histogram Stretch → 마스킹 → np.polyfit 기반 중심 좌표 추정

### `lane_controller.py`

**역할**: 중심 오차 기반 PD 제어
**기능**: 오차 계산 후 회전/전진 속도 계산 → `/cmd_vel`로 명령 송신

### `aruco_tracker.py`

**역할**: 마커 인식 및 거리/방향 추정
**기능**: rvec → 회전행렬 → 오일러 각 변환, tvec 거리 계산

### `pick_and_place.py`

**역할**: 로봇팔 및 그리퍼 제어
**기능**: ArUco 위치 기준으로 이동 및 작업 시퀀스 수행

---

## 5. 도전 과제와 문제 해결 방법

| 문제                | 해결 방안                                                 |
| ----------------- | ----------------------------------------------------- |
| 어두운 환경에서 차선 인식 실패 | CLAHE + Histogram Stretch 결합 → 밝기 및 대비 개선             |
| ArUco 마커 거리 오차    | 카메라 내부 파라미터 정확한 교정 + 3D 거리 정규화                        |
| 프레임 동기화 문제        | ROS2 `message_filters.ApproximateTimeSynchronizer` 사용 |
| 로봇팔 동작 지연         | 비동기 서비스 구조 → 콜백 기반 단계별 실행 안정화                         |

---

## 6. 협업 내용 및 진행 과정

* **협업 내용**

  > 차선 인식 결과와 로봇 주행, ArUco 기반 위치 인식이 하나의 흐름으로 연결되어야 했기 때문에 팀원 간 각 기능의 데이터 연결과 통신 구조 설계가 중요했습니다. 저는 시뮬레이션 환경과 GUI를 설계하며 시각적으로 전체 흐름이 확인 가능하도록 구성하였고, 차선 인식 성능 개선에 주력했습니다.

* **진행 과정**

  * CLAHE, 히스토그램 기반 차선 인식 구조 고도화
  * BEV 변환 행렬 및 관심영역(src/dst) 수치 조정
  * 시뮬레이션 테스트 환경에서 주행 흐름 안정화
  * pick & place 통합 테스트 및 GUI 연동 구조 조정

---

## 7. 성과 및 결과물

* 실시간 차선 인식 → 자율주행 흐름 완성
* ArUco 마커 기반 위치 추정 및 매니퓰레이터 제어 연동 성공
* 밝기/대비 변화에 강한 전처리 구조 확보
* 이미지 정합 기반 프레임 동기화 및 후처리 안정화
* GUI 인터페이스 구성으로 시각화 및 제어 가능

---

## 8. 프로젝트 결과

* **차선 인식 기반 자율주행 + ArUco 작업 수행 전체 시나리오 구현 성공**
* **실제 TurtleBot3 + 매니퓰레이터 플랫폼과 통합 시연 완료**
* **주행, 인식, 조작, GUI가 하나의 파이프라인으로 안정 동작**
* **차선 중심 추출 및 BEV 기반 라인 검출 정확도 향상**

---

## 9. 후기 및 향후 개선 사항

* 다양한 조도 조건에서의 테스트 추가 필요 (야간, 반사 등)
* Pick 위치 예측 정확도 향상을 위한 마커 구조 정교화 필요
* 차선 피팅 알고리즘 보완 및 장애물 회피 로직 확장
* ArUco 감지 시 다중 마커 필터링 로직 추가 고려

---

## 10. 개인적 성찰 및 배운 점

* 차선 인식과 주행은 단순한 문제가 아닌 조도, 반사, 시야 각도 등 다양한 요소를 고려한 설계가 필요함을 체감
* 센서 기반 로봇 제어에서 중요한 것은 **연결된 전체 흐름을 테스트하고 튜닝하는 경험**임을 인식
* 하나의 시스템을 동작시키기 위해 기능별로 역할 분담 → 통합 구조를 어떻게 설계해야 하는지 감각을 익힘

---

## 11. 개선 및 확장 아이디어

* **주행 중 객체 인식 기능 결합**: 주행 중 사물 인식 및 회피
* **다중 마커 분류 및 우선순위 판단 기능**
* **GUI를 모바일 앱으로 확장하여 원격 조작 가능성 실험**
* **실환경 대응을 위한 외부 조도 센서 및 자동 보정 로직 도입**

---
